{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Retrieval Evaluation using Contexts and Ground Truth Contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**: \n",
    "- Novan Parmonangan Simanjuntak (novan.p.simanjuntak@gdplabs.id)\n",
    "\n",
    "\n",
    "**Reviewers**: \n",
    "- Komang Elang Surya Prawira (komang.e.s.prawira@gdplabs.id)\n",
    "- Surya Mahadi (made.r.s.mahadi@gdplabs.id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] [GLAIR GenAI Internal SDK - Evaluate Relevance Between Retrieved Contexts and Ground Truth Contexts](TODO) \\\n",
    "[2] https://github.com/beir-cellar/beir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment\n",
    "Before we start, please make sure to install the SDK library.\n",
    "\n",
    "To install the SDK library, you need to create a personal access token on GitHub. Please follow these steps:\n",
    "1. You need to log in to your [GitHub Account](https://github.com/).\n",
    "2. Go to the [Personal Access Tokens](https://github.com/settings/tokens) page.\n",
    "3. If you haven't created a Personal Access Tokens yet, you can generate one.\n",
    "4. When generating a new token, make sure that you have checked the `repo` option to grant access to private repositories.\n",
    "5. Now, you can copy the new token that you have generated and paste it into the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_sdk_library():\n",
    "    token = getpass.getpass(\"Input Your Personal Access Token: \")\n",
    "\n",
    "    cmd = f\"pip install git+https://{token}:x-oauth-basic@github.com/GDP-ADMIN/gen-ai-internal@f/retrieval_evaluator#egg=gdplabs_gen_ai[eval]\"\n",
    "\n",
    "    with subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) as process:\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            sys.stdout.write(stderr)\n",
    "            raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
    "        else:\n",
    "            sys.stdout.write(stdout)\n",
    "\n",
    "install_sdk_library()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Warning:</b>\n",
    "After running the command above, you need to restart the runtime in Google Colab for the changes to take effect. Not doing so might lead to the newly installed libraries not being recognized.\n",
    "\n",
    "To restart the runtime in Google Colab:\n",
    "- Click on the `Runtime` menu.\n",
    "- Select `Restart runtime`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note: We highly recommend that you run the following code on a GPU instead of a CPU for optimal performance.</b>\n",
    "\n",
    "Simply go to `Runtime` > `Change runtime type` in the menu, select the desired GPU as the `Hardware accelerator`, and hit `Save`. This ensures faster execution of the code. To fine-tune a Large Language Model (LLM) with 7 billion parameters or more without quantization, you will need a minimum of 40 GB of GPU memory, or an equivalent to A100 instances in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Contexts and Ground Truth Contexts\n",
    "Once you have completed the previous step, you can start by importing the necessary library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "from gdplabs_gen_ai.evaluation import EvaluateRetrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then prepare your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth contexts for each query with relevance scores. \n",
    "# Use query_id and context_id to represent query and context.\n",
    "ground_truth_contexts: Dict[str, Dict[str, int]] = {\n",
    "    \"query1\": {\"doc1\": 1, \"doc3\": 1, \"doc5\": 1},\n",
    "    \"query2\": {\"doc2\": 1, \"doc4\": 1},\n",
    "    \"query3\": {\"doc6\": 1, \"doc8\": 1, \"doc10\": 1},\n",
    "    \"query4\": {\"doc7\": 1, \"doc9\": 1}\n",
    "}\n",
    "\n",
    "# Retrieved contexts for each query with similarity scores. It doesn't need to be sorted by scores.\n",
    "# In this examples we retrieve top 5 contexts for each query.\n",
    "retrieved_contexts: Dict[str, Dict[str, float]] = {\n",
    "    \"query1\": {\"doc1\": 0.3, \"doc2\": 0.9, \"doc3\": 0.8, \"doc4\": 0.2, \"doc5\": 0.7},\n",
    "    \"query2\": {\"doc1\": 0.8, \"doc2\": 0.3, \"doc3\": 0.6, \"doc4\": 0.2, \"doc5\": 0.4},\n",
    "    \"query3\": {\"doc6\": 0.5, \"doc7\": 0.3, \"doc8\": 0.7, \"doc9\": 0.2, \"doc10\": 0.6},\n",
    "    \"query4\": {\"doc1\": 0.8, \"doc3\": 0.8, \"doc5\": 0.5, \"doc7\": 0.3, \"doc9\": 0.2}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare list of k values that you want to evaluate (NDCG@k, MAP@k, Recall@k, Precision@k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of k values for evaluation\n",
    "k_values: List[int] = [1, 3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we prepare the dataset and k values, we can start the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate retrieval performance using NDCG@k, MAP@K, Recall@K, and Precision@K\n",
    "ndcg, map_score, recall, precision = EvaluateRetrieval.evaluate(\n",
    "    ground_truth_contexts, retrieved_contexts, k_values\n",
    ")\n",
    "\n",
    "# Display the evaluation metrics\n",
    "print(f\"NDCG: {ndcg}\")\n",
    "print(f\"MAP: {map_score}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results\n",
    "\n",
    "The output provides key insights into the performance of the retrieval system. Let's break down each metric and interpret what it signifies:  \n",
    "Normalized Discounted Cumulative Gain (NDCG):  \n",
    "NDCG@1: 0.25  \n",
    "NDCG@3: 0.38268  \n",
    "NDCG@5: 0.68384  \n",
    "Interpretation: NDCG measures the quality of the ranked retrieved documents based on their relevance. The values indicate that the relevance improves as more documents are considered (from 1 to 5). A score of 0.68384 at NDCG@5 suggests that the top 5 documents have a moderately high relevance. However, the lower score at NDCG@1 indicates that the most relevant document is not always ranked highest.  \n",
    "\n",
    "\n",
    "Mean Average Precision (MAP):  \n",
    "MAP@1: 0.08333  \n",
    "MAP@3: 0.34722  \n",
    "MAP@5: 0.57222  \n",
    "Interpretation: MAP assesses the precision across all queries. The scores reflect a similar trend as NDCG, where precision improves with more documents. The low score at MAP@1 implies that the top-ranked document is often not the most relevant. However, considering more documents (up to 5) increases the likelihood of including relevant documents.  \n",
    "\n",
    "\n",
    "Recall:  \n",
    "Recall@1: 0.08333  \n",
    "Recall@3: 0.41667  \n",
    "Recall@5: 1.0  \n",
    "Interpretation: Recall measures how many relevant documents are retrieved. The perfect score at Recall@5 indicates that all relevant documents are included within the top 5. However, the lower scores at Recall@1 and Recall@3 suggest that not all relevant documents are ranked at the very top.\n",
    "Precision:  \n",
    "P@1: 0.25  \n",
    "P@3: 0.41667  \n",
    "P@5: 0.5  \n",
    "Interpretation: Precision evaluates how many of the retrieved documents are relevant. The scores show that while half of the top 5 documents are relevant (P@5), the precision at the very top (P@1) is lower.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action based on metrics:  \n",
    "\n",
    "\n",
    "1. Focus on Improving Top-Ranked Results: The lower scores at NDCG@1 and MAP@1 suggest a need to enhance the algorithm to ensure the most relevant documents are ranked higher.  \n",
    "2. Balance Between Precision and Recall: While recall at 5 is perfect, precision is only 50%, indicating a trade-off. Aim to improve precision without significantly compromising recall.  \n",
    "3. Analyze Individual Queries: Investigate queries where the retrieval system underperforms, especially those contributing to lower scores at lower 'k' values.  \n",
    "4. Refine Retrieval Algorithms: Consider adjusting or fine-tuning the retrieval algorithm, possibly integrating more sophisticated techniques like semantic search or machine learning models that can better understand query intent.  \n",
    "5. Iterative Testing and Evaluation: Continuously evaluate and refine the system using different datasets and queries to ensure robustness and versatility.  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
