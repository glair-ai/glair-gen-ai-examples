{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa25d5c",
   "metadata": {
    "id": "afa25d5c"
   },
   "source": [
    "# Example of End-To-End Retrieval Augmented Generation Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af2dd95",
   "metadata": {
    "id": "2af2dd95"
   },
   "source": [
    "**Authors**:\n",
    "- Henry Wicaksono (henry.wicaksono@gdplabs.id)\n",
    "\n",
    "**Reviewers**:\n",
    "- Kevin Yauris (kevin.yauris@gdplabs.id)\n",
    "- Timotius Nugraha Chandra (timotius.n.chandra@gdplabs.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2297bd3",
   "metadata": {
    "id": "d2297bd3"
   },
   "source": [
    "## References\n",
    "\n",
    "[1] [GDP Labs Gen AI SDK - Document Processing Orchestrator: Parser Chunker](https://docs.glair.ai/generative-internal/modules/document-processing-orchestrator/parser-chunker)  \n",
    "[2] [GDP Labs Gen AI SDK - Document Processing Orchestrator: Metadata Generator](https://docs.glair.ai/generative-internal/modules/document-processing-orchestrator/metadata-generator)  \n",
    "[3] [GDP Labs Gen AI SDK - Document Processing Orchestrator: Indexer](https://docs.glair.ai/generative-internal/modules/document-processing-orchestrator/indexer)  \n",
    "[4] [GDP Labs Gen AI SDK - Retrieval: Retriever](https://docs.glair.ai/generative-internal/modules/retrieval/retriever)  \n",
    "[5] [GDP Labs Gen AI SDK - Inference Orchestrator: LLM](https://docs.glair.ai/generative-internal/modules/inference-orchestrator/model-io/llm)  \n",
    "[6] [GDP Labs Gen AI SDK - Inference Orchestrator: Prompt Builder](https://docs.glair.ai/generative-internal/modules/inference-orchestrator/model-io/prompt-builder)  \n",
    "[7] [GDP Labs Gen AI SDK - Inference Orchestrator: Use Case Handler](https://docs.glair.ai/generative-internal/modules/inference-orchestrator/use-case-handler)  \n",
    "[8] [GDP Labs Gen AI SDK - Inference Orchestrator: Flow Executor](https://docs.glair.ai/generative-internal/modules/inference-orchestrator/flow-executor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ef20e-60c2-4395-a52d-6f9f461e32ee",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this example, we're going to use the following GDP Labs Gen AI SDK modules to perform end-to-end Retrieval Augmented Generation (RAG):\n",
    "- [Document Processing Orchestrator](https://docs.glair.ai/generative-internal/modules/document-processing-orchestrator): To process document and index them into vector databases.\n",
    "- [Retrieval](https://docs.glair.ai/generative-internal/modules/retrieval): To retrieve knowledge from a certain source to be used in the RAG flow.\n",
    "- [Inference Orchestrator](https://docs.glair.ai/generative-internal/modules/inference-orchestrator): To perform the inference in the RAG flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025185a",
   "metadata": {
    "id": "3025185a"
   },
   "source": [
    "# Prepare Environment\n",
    "\n",
    "Before we start, ensure you have a GitHub account with access to the GDP Labs GenAI SDK GitHub repository. Then, follow these steps to create a personal access token:\n",
    "1. Log in to your [GitHub](https://github.com/) account.\n",
    "2. Navigate to the [Personal Access Tokens](https://github.com/settings/tokens) page.\n",
    "3. Select the `Generate new token` option. You can use the classic version instead of the beta version.\n",
    "4. Fill in the required information, ensuring that you've checked the `repo` option to grant access to private repositories.\n",
    "5. Save the newly generated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3d2b69f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3d2b69f",
    "outputId": "11041de7-71e8-4a41-abea-98fba06f14f5"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input Your Personal Access Token:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_sdk_library() -> None:\n",
    "    \"\"\"Installs the `gdplabs_gen_ai` library from a private GitHub repository using a Personal Access Token.\n",
    "\n",
    "    This function prompts the user to input their Personal Access Token for GitHub authentication. It then constructs\n",
    "    the repository URL with the provided token and executes a subprocess to install the library via pip from the\n",
    "    specified repository.\n",
    "\n",
    "    Raises:\n",
    "        subprocess.CalledProcessError: If the installation process returns a non-zero exit code.\n",
    "\n",
    "    Note:\n",
    "        The function utilizes `getpass.getpass()` to securely receive the Personal Access Token without echoing it.\n",
    "    \"\"\"\n",
    "    token = getpass.getpass(\"Input Your Personal Access Token: \")\n",
    "    repo_url_with_token = f\"https://{token}@github.com/GDP-ADMIN/gen-ai-internal.git\"\n",
    "    cmd = [\"pip\", \"install\", f\"gdplabs_gen_ai[eval] @ git+{repo_url_with_token}\", \"-q\"]\n",
    "\n",
    "    try:\n",
    "        with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                              text=True, bufsize=1, universal_newlines=True) as process:\n",
    "            for line in process.stdout:\n",
    "                sys.stdout.write(line)\n",
    "\n",
    "            process.wait()  # Wait for the process to complete.\n",
    "            if process.returncode != 0:\n",
    "                raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}.\")\n",
    "\n",
    "install_sdk_library()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81baeb",
   "metadata": {
    "id": "fa81baeb"
   },
   "source": [
    "<b>Warning:</b>\n",
    "After running the command above, you need to restart the runtime in Google Colab for the changes to take effect. Not doing so might lead to the newly installed libraries not being recognized.\n",
    "\n",
    "To restart the runtime in Google Colab:\n",
    "- Click on the `Runtime` menu.\n",
    "- Select `Restart runtime`.\n",
    "\n",
    "Once you have completed the previous step, you are ready to start using the SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0c2f04-4765-4cc1-a2ff-d01a5e57479f",
   "metadata": {},
   "source": [
    "# Set Up OpenAI API Key\n",
    "\n",
    "Since we're going to use an OpenAI model in this example, we'd need to set up an OpenAI API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31qPR6B3095T",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "31qPR6B3095T",
    "outputId": "28520450-ea60-434e-af3f-547f69eefc73"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input Your OpenAI API Key:  ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Input Your OpenAI API Key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da25237",
   "metadata": {
    "id": "2da25237"
   },
   "source": [
    "# Processing Document Using Document Processing Orchestrator Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6100712-bae4-45b3-a321-39bdebf305ec",
   "metadata": {},
   "source": [
    "First, we're going to utilize the interfaces in the `Document Processing Orchestrator` modules to process a document and index it to a vector database. In this example, we'll be using [LangChain's UnstructuredPDFLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.pdf.UnstructuredPDFLoader.html#) to load the document. Let's start by installing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "NLoHO_z_nlkM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NLoHO_z_nlkM",
    "outputId": "0da9d0e2-37b3-4db3-8b0b-d7d44b147876"
   },
   "outputs": [],
   "source": [
    "!pip install unstructured==0.10.16 -q\n",
    "!pip install pdfminer.six -q\n",
    "!pip install pdf2image -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c539b-6123-49d2-9a64-d884233bc119",
   "metadata": {},
   "source": [
    "Then, let's prepare the documents. In this example, we're going to use a PDF file called `gdplabs.pdf` that contains information about GDP Labs. You can find it in the [data/rag](https://github.com/glair-ai/glair-gen-ai-examples/tree/main/notebooks/data/rag) folder. Then, let's also define some of the paths that we're going to need later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc72aad",
   "metadata": {
    "id": "cfc72aad"
   },
   "outputs": [],
   "source": [
    "DOCUMENT_PATH = \"data/rag/gdplabs.pdf\"\n",
    "CSV_PATH = \"data/rag/gdplabs.csv\"\n",
    "CHROMA_PATH = \"data/rag/gdplabs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee995f-892a-4b95-b4cf-5e03c23776c0",
   "metadata": {},
   "source": [
    "### Process Document Into Chunks With ParserChunker\n",
    "\n",
    "Let's start working with the `Document Processing Orchestrator`. Here, we're creating an implementation of the [ParserChunker](https://docs.glair.ai/generative-internal/modules/document-processing-orchestrator/parser-chunker) interface that will load the document, parse the contents, split them into chunks, and store them as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dcf5166",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5dcf5166",
    "outputId": "c48a37de-9d17-49ae-a90b-ab9ffab37c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed and chunked document!\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from gdplabs_gen_ai.document_processing_orchestrator.parser_chunker import BaseParserChunker\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "import pandas as pd\n",
    "\n",
    "class ParserChunker(BaseParserChunker):\n",
    "    def parse_chunk(self, path_input: str, path_output: str) -> None:\n",
    "        parsed_text = self._parse(path_input)\n",
    "        chunk_list = self._chunk(parsed_text)\n",
    "\n",
    "        df = pd.DataFrame(data={\"chunk\": chunk_list})\n",
    "        df.to_csv(path_output, index=False)\n",
    "        print(\"Successfully parsed and chunked document!\")\n",
    "\n",
    "    def _parse(self, path_input: str) -> str:\n",
    "        loader = UnstructuredPDFLoader(path_input)\n",
    "        page_content_list = [doc.page_content for doc in loader.load()]\n",
    "        return \"\\n---\\n\".join(page_content_list)\n",
    "\n",
    "    def _chunk(self, parsed_text: str) -> List[str]:\n",
    "        chunk_list = parsed_text.split(\"\\n---\\n\")\n",
    "        title = chunk_list[0].strip()\n",
    "        return [f\"{title}\\n{chunk.strip()}\" for chunk in chunk_list[1:]]\n",
    "\n",
    "parser_chunker = ParserChunker()\n",
    "parser_chunker.parse_chunk(DOCUMENT_PATH, CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d324938c-63f2-4382-ab5b-558eec455c7a",
   "metadata": {},
   "source": [
    "### Add Metadata With MetadataGenerator\n",
    "\n",
    "Next, we're utilizing an implementation of the [MetadataGenerator](https://docs.glair.ai/generative-internal/modules/document-processing-orchestrator/metadata-generator) interface that will add metadata to the chunks. Here, we're adding `chunk_id`, `chunk_size`, `prev_chunk_id`, and `next_chunk_id` as the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f45b1e15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f45b1e15",
    "outputId": "4de2fa27-2da6-4666-902f-0b10c5029750"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated metadata!\n"
     ]
    }
   ],
   "source": [
    "from gdplabs_gen_ai.document_processing_orchestrator.metadata_generator import BaseMetadataGenerator\n",
    "from uuid import uuid4\n",
    "import tiktoken\n",
    "\n",
    "class MetadataGenerator(BaseMetadataGenerator):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "    def generate_metadata(self, path_input: str, path_output: str) -> None:\n",
    "      chunk_list = pd.read_csv(path_input).chunk.tolist()\n",
    "      chunk_size_list = [self._count_token(chunk) for chunk in chunk_list]\n",
    "      chunk_id_list = [str(uuid4()) for chunk in chunk_list]\n",
    "      prev_chunk_id_list = [\"-\"] + chunk_id_list[:-1]\n",
    "      next_chunk_id_list = chunk_id_list[1:] + [\"-\"]\n",
    "\n",
    "      df = pd.DataFrame(data={\n",
    "          \"chunk_id\": chunk_id_list,\n",
    "          \"chunk\": chunk_list,\n",
    "          \"chunk_size\": chunk_size_list,\n",
    "          \"prev_chunk_id\": prev_chunk_id_list,\n",
    "          \"next_chunk_id\": next_chunk_id_list,\n",
    "      })\n",
    "      df.to_csv(path_output, index=False)\n",
    "      print(\"Successfully generated metadata!\")\n",
    "\n",
    "    def _count_token(self, text: str) -> int:\n",
    "        return len(self.encoding.encode(text))\n",
    "\n",
    "metadata_generator = MetadataGenerator()\n",
    "metadata_generator.generate_metadata(CSV_PATH, CSV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd81a09-96ed-4ba3-ba7e-823c0543630f",
   "metadata": {},
   "source": [
    "### Index Into Vector Database With Indexer\n",
    "\n",
    "Last, we're going to insert the chunks along with the generated metadata into a vector database. We can do this by creating an implementation of the [Indexer](https://docs.glair.ai/generative-internal/modules/document-processing-orchestrator/indexer) interface. For simplicity sake, let's use [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) vector database in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81091456",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81091456",
    "outputId": "c6e48ca3-3b3d-4337-f901-05767cf20ccf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully indexed data to vector database!\n"
     ]
    }
   ],
   "source": [
    "from gdplabs_gen_ai.document_processing_orchestrator.indexer import BaseIndexer\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "class Indexer(BaseIndexer):\n",
    "    def index(self, path_input: str, db_url: str, index_name: str) -> None:\n",
    "        df = pd.read_csv(path_input)\n",
    "        documents = []\n",
    "        for _, row in df.iterrows():\n",
    "            documents.append(Document(\n",
    "                page_content=row[\"chunk\"],\n",
    "                metadata={\n",
    "                    \"chunk_id\": row[\"chunk_id\"],\n",
    "                    \"chunk_size\": row[\"chunk_size\"],\n",
    "                    \"prev_chunk_id\": row[\"prev_chunk_id\"],\n",
    "                    \"next_chunk_id\": row[\"next_chunk_id\"],\n",
    "                }\n",
    "            ))\n",
    "        embeddings = OpenAIEmbeddings()\n",
    "        chroma = Chroma.from_documents(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            collection_name=index_name,\n",
    "            persist_directory=db_url,\n",
    "        )\n",
    "        print(\"Successfully indexed data to vector database!\")\n",
    "\n",
    "    def delete_document(self, db_url: str, index_name: str, document_ids: List[str]) -> None:\n",
    "        pass\n",
    "\n",
    "indexer = Indexer()\n",
    "indexer.index(CSV_PATH, CHROMA_PATH, \"gdplabs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GKo-GZ6g20CL",
   "metadata": {
    "id": "GKo-GZ6g20CL"
   },
   "source": [
    "# Retrieving Data Using Retrieval Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fed35-661b-4f4b-9a8f-ea49b4371044",
   "metadata": {},
   "source": [
    "After we've successfully indexed the data into the vector database, we can utilize the [VectorDBSimilaritySearchRetriever](https://docs.glair.ai/generative-internal/modules/retrieval/retriever#vector-db-similarity-search-retriever) class to retrieve them. This class retrieves chunks from a vector database based on their semantic similarity with the given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee5da3a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ee5da3a6",
    "outputId": "0079fa88-a16c-4a33-839a-6c20173bec43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDP Labs\n",
      "Part of Us\n",
      "\n",
      "1. Visit CATAPA\n",
      "\n",
      "CATAPA Intelligent Payroll Platform CATAPA 3S Payroll Platform is a Swift, Simple, and Secure Payroll Solution that is specifically designed to help you run your company payroll to be 15x more efficient. Say goodbye to time-wasting payroll process and save up to 12.000 minutes of your time in a year with a fast 1 minute payroll process for 300 employees*. Focus on things that truly matter for your business with CATAPA as your company payroll platform choice.\n",
      "\n",
      "2. Visit GLAIR\n",
      "\n",
      "GLAIR - Accelerate Digital Transformation One-stop technology consulting services in Indonesia. GLAIR specialise in Artificial Intelligence, Blockchain, Cloud, Data, Mobile, Web, and Security and ready to help your company embrace the next wave of technologies\n",
      "\n",
      "a. GLAIR Analytics\n",
      "\n",
      "A simple & intelligent way to turn your data into insights. Using Artificial Intelligence combined with our expertise in big data to discover valuable insights from your data b. GLAIR Consulting\n",
      "\n",
      "Worked together with some of the best providers in the world like AWS and Google Cloud to deliver the most suitable Artificial Intelligence, Blockchain, Cloud, Machine Learning, and Big Data powered systems to our customers.\n",
      "\n",
      "3. GLx - Research Institute\n",
      "\n",
      "Both learning and research centre that focus on investigate leading and bleeding edge technologies and tools, as well as provide training to our employee\n",
      "\n",
      "4. Visit KASKUS\n",
      "\n",
      "Kaskus Development Team Kaskus is the first and largest Hobby & Community Sharing place in Indonesia. KASKUS provides space for anyone to express their true side.\n"
     ]
    }
   ],
   "source": [
    "from gdplabs_gen_ai.retrieval.retriever.vector_db_similarity_search_retriever import VectorDBSimilaritySearchRetriever\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "chroma = Chroma(\n",
    "    persist_directory=CHROMA_PATH,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"gdplabs\"\n",
    ")\n",
    "retriever = VectorDBSimilaritySearchRetriever(chroma)\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"What does GLAIR specialize in?\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "B6edEOpjLUvn",
   "metadata": {
    "id": "B6edEOpjLUvn"
   },
   "source": [
    "# Performing Inference Using Inference Orchestrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f041e8-a1b4-4828-93c5-f7f90e9bd7b9",
   "metadata": {},
   "source": [
    "Finally, to perform an RAG inference, we can use the retriever object we've just created along with the following [Inference Orchestrator](https://docs.glair.ai/generative-internal/modules/inference-orchestrator) modules:\n",
    "1. [ChatOpenAILLM](https://docs.glair.ai/generative-internal/modules/inference-orchestrator/model-io/llm#chat-open-aillm): To utilize OpenAI's chat models to generate response. The default model used is `gpt-3.5-turbo`.\n",
    "2. [PromptBuilder](https://docs.glair.ai/generative-internal/modules/inference-orchestrator/model-io/prompt-builder#prompt-builder): To manage prompt templates and format them before sending them to the LLM.\n",
    "3. [QAUseCaseHandler](https://docs.glair.ai/generative-internal/modules/inference-orchestrator/use-case-handler#qa-use-case-handler): To handle RAG-like flow where we retrieve knowledge from a certain source and uses it as an additional context.\n",
    "4. [FlowExecutor](https://docs.glair.ai/generative-internal/modules/inference-orchestrator/flow-executor): To wrap the other components and executes the whole flow. It enables easier streaming management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60b33349",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60b33349",
    "outputId": "00dc01fd-5e41-4ded-99f3-18aa0dc6f54e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-01-05 10:29:29 +0700] [13759] [INFO] Processing the message into the use case flow...\n",
      "[2024-01-05 10:29:29 +0700] [13759] [INFO] Searching for relevant documents in the topic_1 collection.\n",
      "[2024-01-05 10:29:29 +0700] [13759] [INFO] Found 4 most relevant documents.\n",
      "[2024-01-05 10:29:29 +0700] [13759] [INFO] Enough documents are retrieved. Using 1 of the most relevant documents to answer the question.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLAIR specializes in Artificial Intelligence, Blockchain, Cloud, Data, Mobile, Web, and Security."
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "\n",
    "from gdplabs_gen_ai.inference_orchestrator.llm import ChatOpenAILLM, Generator\n",
    "from gdplabs_gen_ai.inference_orchestrator.prompt import PromptBuilder\n",
    "from gdplabs_gen_ai.inference_orchestrator.use_case import QAUseCaseHandler\n",
    "from gdplabs_gen_ai.inference_orchestrator import FlowExecutor\n",
    "\n",
    "llm = ChatOpenAILLM()\n",
    "prompt_builder = PromptBuilder.from_template(\"answer_question\")\n",
    "retriever_map = {\"topic_1\": retriever}\n",
    "configs = {\"model_max_context_token\": 400}\n",
    "\n",
    "use_case_handler = QAUseCaseHandler(retriever_map, llm, prompt_builder, configs=configs)\n",
    "flow_executor = FlowExecutor(use_case_handler)\n",
    "\n",
    "generator = Generator()\n",
    "message = \"What does GLAIR specialize in?\"\n",
    "thread = Thread(target=flow_executor.run_flow, args=(generator, message, \"topic_1\"))\n",
    "thread.start()\n",
    "thread.join()\n",
    "\n",
    "for token in generator:\n",
    "    print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb196780-5a9c-4cf5-a541-40ffd1e2ab58",
   "metadata": {
    "id": "ivI6FM8BLZeC"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61a639-cb47-4edf-9477-0189fed3bfbd",
   "metadata": {},
   "source": [
    "In this example, we've learned how to use the GDP Labs Gen AI SDK's `Document Processing Orchestrator`, `Retrieval`, and `Inference Orchestrator` modules to perform an end-to-end Retrieval Augmented Generation flow."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "example",
   "language": "python",
   "name": "example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
