{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Ragas's Context Precision and Context Recall Metrics\n",
    "### Using Ragas's Metrics to Evaluate Your Retrieval Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**: \n",
    "- Komang Elang Surya Prawira (komang.e.s.prawira@gdplabs.id)\n",
    "\n",
    "**Reviewers**: \n",
    "- Novan Parmonangan Simanjuntak (novan.p.simanjuntak@gdplabs.id)\n",
    "- Surya Mahadi (made.r.s.mahadi@gdplabs.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Lorem ipsum. \\\n",
    "[2] Lorem ipsum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Using LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore how to evaluate the performance of our retrieval when we don't have any ground truth contexts available as references. As a result, we will leverage LLM to evaluate the retrieved contexts. Below is the data needed to perform the evaluation when using LLM:\n",
    "1. Questions (List[str]): A list of questions.\n",
    "2. Retrieved Contexts (List[List[str]]): Contexts retrieved for each question.\n",
    "3. Ground Truth Responses (List[str]): Ground truth responses for each question.\n",
    "\n",
    "We recommend two metrics for you to use, which are Context Precision and Context Recall.\n",
    "1. **Context Precision** measures the extent to which the retrieved contexts are relevant to the given question.\n",
    "2. **Context Recall** measures the extent to which the ground truth responses are reflected (mentioned) in the retrieved contexts.\n",
    "\n",
    "As stated in each corresponding description, **Context Precision** requires `Questions` and `Retrieved Contexts`, whereas **Context Recall** requires `Ground Truth Responses` and `Retrieved Contexts`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Environment\n",
    "\n",
    "Before we start, ensure you have a GitHub account with access to the GLAIR GenAI Internal SDK GitHub repository. Then, follow these steps to create a personal access token:\n",
    "1. Log in to your [GitHub](https://github.com/) account.\n",
    "2. Navigate to the [Personal Access Tokens](https://github.com/settings/tokens) page.\n",
    "3. Select the `Generate new token` option. You can use the classic version instead of the beta version.\n",
    "4. Fill in the required information, ensuring that you've checked the `repo` option to grant access to private repositories.\n",
    "5. Save the newly generated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_sdk_library() -> None:\n",
    "    \"\"\"Installs the `glair_genai_sdk` library from a private GitHub repository using a Personal Access Token.\n",
    "\n",
    "    This function prompts the user to input their Personal Access Token for GitHub authentication. It then constructs\n",
    "    the repository URL with the provided token and executes a subprocess to install the library via pip from the\n",
    "    specified repository.\n",
    "\n",
    "    Raises:\n",
    "        subprocess.CalledProcessError: If the installation process returns a non-zero exit code.\n",
    "\n",
    "    Note:\n",
    "        The function utilizes `getpass.getpass()` to securely receive the Personal Access Token without echoing it.\n",
    "    \"\"\"\n",
    "    token = getpass.getpass(\"Input Your Personal Access Token: \")\n",
    "    repo_url_with_token = f\"https://{token}@github.com/GDP-ADMIN/gen-ai-internal-sdk.git\"\n",
    "    cmd = [\"pip\", \"install\", \"-e\", f\"git+{repo_url_with_token}#egg=glair_genai_sdk\"]\n",
    "\n",
    "    try:\n",
    "        with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                              text=True, bufsize=1, universal_newlines=True) as process:\n",
    "            for line in process.stdout:\n",
    "                sys.stdout.write(line)\n",
    "\n",
    "            process.wait()  # Wait for the process to complete\n",
    "            if process.returncode != 0:\n",
    "                raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "install_sdk_library()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Warning:</b>\n",
    "After running the command above, you need to restart the runtime in Google Colab for the changes to take effect. Not doing so might lead to the newly installed libraries not being recognized.\n",
    "\n",
    "To restart the runtime in Google Colab:\n",
    "- Click on the `Runtime` menu.\n",
    "- Select `Restart runtime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR-API-KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    ContextPrecision,\n",
    "    ContextRecall,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from typing import List, Optional\n",
    "\n",
    "def convert_to_hf_dataset(questions: List[str], retrieved_contexts: List[List[str]], \n",
    "                          ground_truth_responses: Optional[List[str]] = None) -> Dataset:\n",
    "    \"\"\"Converts provided data into a Hugging Face Dataset format.\n",
    "\n",
    "    Args:\n",
    "        questions (List[str]): A list of questions.\n",
    "        retrieved_contexts (List[List[str]]): Contexts retrieved for each question.\n",
    "        ground_truth_responses (Optional[List[str]]): Ground truth responses for each question. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Dataset: A Hugging Face `Dataset` object containing the organized data.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the lengths of the provided lists are inconsistent.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for consistent lengths.\n",
    "    lengths = [len(questions), len(retrieved_contexts)]\n",
    "    if ground_truth_responses is not None:\n",
    "        lengths.append(len(ground_truth_responses))\n",
    "\n",
    "    if len(set(lengths)) > 1:\n",
    "        raise ValueError(\"All input lists must be of the same length.\")\n",
    "\n",
    "    data = {\n",
    "        'questions': questions,\n",
    "        'retrieved_contexts': retrieved_contexts\n",
    "    }\n",
    "\n",
    "    if ground_truth_responses is not None:\n",
    "        data['ground_truth_responses'] = ground_truth_responses\n",
    "\n",
    "    # Convert to Hugging Face Dataset.\n",
    "    dataset = Dataset.from_dict(data)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepared Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gdplabs_gen_ai.evaluation.utility import convert_to_hf_dataset\n",
    "\n",
    "# Define your data here before converting it into a Hugging Face's `Dataset` object.\n",
    "questions = [\"What is AI?\", \"Who is Elon Musk?\"]\n",
    "retrieved_contexts = [[\"AI is dangerous\", \"Artificial Intelligence is hard to master\"], [\"Elon Musk is rich\", \"CEO of SpaceX is Elon Musk\"]]\n",
    "ground_truth_responses = [[\"A field of computer science\"], [\"An entrepreneur and business magnate\"]]\n",
    "\n",
    "dataset = convert_to_hf_dataset(questions, retrieved_contexts, ground_truth_responses)\n",
    "print(dataset)\n",
    "# Dataset({\n",
    "#     features: ['questions', 'retrieved_contexts', 'ground_truth_responses'],\n",
    "#     num_rows: 2\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Precision & Context Recall Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Default LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_precision = ContextPrecision(\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "context_recall = ContextRecall(\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    column_map={\"question\": \"questions\", \"contexts\": \"retrieved_contexts\", \"ground_truths\": \"ground_truth_responses\"},\n",
    ")\n",
    "\n",
    "print(score)\n",
    "# evaluating with [context_precision]\n",
    "# 100%|██████████| 1/1 [00:01<00:00,  1.51s/it]\n",
    "# evaluating with [context_recall]\n",
    "# 100%|██████████| 1/1 [00:02<00:00,  2.20s/it]\n",
    "# {'context_precision': 0.2500, 'context_recall': 0.0000}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bring Your Own LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI via LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from ragas.llms import LangchainLLM\n",
    "\n",
    "gpt4 = ChatOpenAI(model_name=\"gpt-4\")\n",
    "gpt4_wrapper = LangchainLLM(llm=gpt4)\n",
    "\n",
    "print(f\"LLM used by Context Precision before customization: {context_precision.llm }\")\n",
    "# LLM used by Context Precision before customization: OpenAI(model='gpt-3.5-turbo-16k', _api_key_env_var='OPENAI_API_KEY')\n",
    "\n",
    "context_precision.llm = gpt4_wrapper\n",
    "context_recall.llm = gpt4_wrapper\n",
    "\n",
    "print(f\"LLM used by Context Precision after customization: {context_precision.llm }\")\n",
    "# LLM used by Context Precision after customization: <ragas.llms.langchain.LangchainLLM object at 0x7f8acf4dca00>\n",
    "\n",
    "score_gpt4 = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    "    column_map={\"question\": \"questions\", \"contexts\": \"retrieved_contexts\", \"ground_truths\": \"ground_truth_responses\"},\n",
    ")\n",
    "\n",
    "print(score_gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Generation Inference (TGI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "from ragas.llms import LangchainLLM\n",
    "\n",
    "tgi = HuggingFaceTextGenInference(\n",
    "    inference_server_url=\"http://localhost:8010/\",\n",
    "    max_new_tokens=512,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    temperature=0.01,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "tgi_wrapper = LangchainLLM(llm=tgi)\n",
    "\n",
    "print(f\"LLM used by Context Precision before customization: {context_precision.llm }\")\n",
    "# LLM used by Context Precision before customization: OpenAI(model='gpt-3.5-turbo-16k', _api_key_env_var='OPENAI_API_KEY')\n",
    "\n",
    "context_precision.llm = tgi_wrapper\n",
    "context_recall.llm = tgi_wrapper\n",
    "\n",
    "print(f\"LLM used by Context Precision after customization: {context_precision.llm }\")\n",
    "# LLM used by Context Precision after customization: <ragas.llms.langchain.LangchainLLM object at 0x7f8acf4dca00>\n",
    "\n",
    "score_tgi = evaluate(\n",
    "    dataset,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(score_tgi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
