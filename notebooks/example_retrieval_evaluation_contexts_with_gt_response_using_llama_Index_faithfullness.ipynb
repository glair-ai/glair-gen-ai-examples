{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwABn_XL78IY"
      },
      "source": [
        "# Example of LlamaIndex Faithfullness Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caw7ngLl8C8U"
      },
      "source": [
        "**Authors**:\n",
        "- Surya Mahadi (made.r.s.mahadi@gdplabs.id)\n",
        "\n",
        "**Reviewers**:\n",
        "- Novan Parmonangan Simanjuntak (novan.p.simanjuntak@gdplabs.id)\n",
        "- Komang Elang Surya Prawira (komang.e.s.prawira@gdplabs.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFpk43UE8Pnd"
      },
      "source": [
        "## References\n",
        "[1] [GLAIR GenAI Internal SDK - LlamaIndex Faithfullness Evaluation](#) \\\n",
        "[2] [LLamaIndex - Faithfullness Evaluation](https://docs.llamaindex.ai/en/stable/examples/evaluation/faithfulness_eval.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpnjCv8wCBtm"
      },
      "source": [
        "# Prepare Environment\n",
        "\n",
        "Before we start, please make sure to install the SDK library and download the Truthful QA dataset to your local file system.\n",
        "\n",
        "To install the SDK library, you need to create a personal access token on GitHub. Please follow these steps:\n",
        "1. You need to log in to your [GitHub Account](https://github.com/).\n",
        "2. Go to the [Personal Access Tokens](https://github.com/settings/tokens) page.\n",
        "3. If you haven't created a Personal Access Tokens yet, you can generate one.\n",
        "4. When generating a new token, make sure that you have checked the `repo` option to grant access to private repositories.\n",
        "5. Now, you can copy the new token that you have generated and paste it into the script below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D94Nd7mE9_An"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_sdk_library():\n",
        "    token = getpass.getpass(\"Input Your Personal Access Token: \")\n",
        "\n",
        "    cmd = f\"pip install -e git+https://{token}@github.com/GDP-ADMIN/glair-genai-experiments-and-explorations.git#egg=glair_genai_sdk\"\n",
        "\n",
        "    with subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True) as process:\n",
        "        stdout, stderr = process.communicate()\n",
        "\n",
        "        if process.returncode != 0:\n",
        "            sys.stdout.write(stderr)\n",
        "            raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
        "        else:\n",
        "            sys.stdout.write(stdout)\n",
        "\n",
        "install_sdk_library()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gnTPpxPCeP0"
      },
      "source": [
        "<b>Warning:</b>\n",
        "After running the command above, you need to restart the runtime in Google Colab for the changes to take effect. Not doing so might lead to the newly installed libraries not being recognized.\n",
        "\n",
        "To restart the runtime in Google Colab:\n",
        "- Click on the `Runtime` menu.\n",
        "- Select `Restart runtime`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-mbsYlCgiO"
      },
      "source": [
        "# Faithfullness Evaluation\n",
        "Once you have completed the previous step, you are ready to import the library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i3TCiPGz925V"
      },
      "outputs": [],
      "source": [
        "from llama_index import ServiceContext\n",
        "from llama_index.llms import OpenAI\n",
        "from gdplabs_gen_ai.evaluation import FaithfulnessEvaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvFcpSOYCrJa"
      },
      "source": [
        "after that, you need to prepare your data in the following format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zs5VLIf496Jq"
      },
      "outputs": [],
      "source": [
        "ground_truth_responses = [\n",
        "    \"AI is artificial intelligence\",\n",
        "    \"Car is and transportation\",\n",
        "]\n",
        "\n",
        "retrieved_contexts = [\n",
        "    [\"Today AI is used everywhere\", \"AI was first developed on 1970, AI stands for Artificial Intelligence\"],\n",
        "    [\"Toyota is a car factory that success in Japan\", \"Today lot of people use car as their main transportation\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UULaHhT6C2UW"
      },
      "source": [
        "Next we need to create our LLM, in this example we will use GPT4 as the LLM. Remember to put your `OPENAI_API_KEY` into the env variable, you can use `os.environment` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PF5X3okm-egG"
      },
      "outputs": [],
      "source": [
        "# create service context\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
        "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
        "\n",
        "# create evaluator\n",
        "evaluator_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTwxo3UHDE0w"
      },
      "source": [
        "Finally we can calculate the faithfullness score using the following code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eMkBIVfx__z-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "scores = []\n",
        "\n",
        "for ground_truth_response, retrieved_context in zip(ground_truth_responses, retrieved_contexts):\n",
        "  # there are 2 API for evaluation, sync and async\n",
        "  # in this example we use the async version\n",
        "  result = await evaluator_gpt4.aevaluate(response=ground_truth_response, contexts=retrieved_context)\n",
        "  # if you want to use the sync version, you can use the following code\n",
        "  # result = evaluator_gpt4.evaluate(response=ground_truth_response, contexts=retrieved_context)\n",
        "  scores.append(int(result.passing))\n",
        "\n",
        "print(f\"Score: {sum(scores)/len(scores)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMfeCsqVDLrl"
      },
      "source": [
        "The above example will calculate each response-context pair, either `PASS` or `NOT PASS` and than calculate the mean score\n",
        "\n",
        "**note**: Since in this example we use jupyter notebook, and internally [jupyter notebook already running an event loop](https://blog.jupyter.org/ipython-7-0-async-repl-a35ce050f7f7), you can only use the async version here. If you want to use the sync version, use it outside jupyter notebook"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
