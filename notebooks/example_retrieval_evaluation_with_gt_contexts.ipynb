{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Evaluate Relevance Between Retrieved Contexts and Ground Truth Contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors**: \n",
    "- Novan Parmonangan Simanjuntak (novan.p.simanjuntak@gdplabs.id)\n",
    "\n",
    "**Reviewers**: \n",
    "- Komang Elang Surya Prawira (komang.e.s.prawira@gdplabs.id)\n",
    "- Surya Mahadi (made.r.s.mahadi@gdplabs.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] [GDP Labs GenAI SDK - Evaluate Relevance Between Retrieved Contexts and Ground Truth Contexts](https://docs.glair.ai/generative-internal/modules/evaluator/cookbook/retrieval-evaluator/retrieval-evaluation-methods/evaluate-relevance-between-retrieved-contexts-and-ground-truth-contexts) \\\n",
    "[2] [BEIR](https://github.com/beir-cellar/beir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Environment\n",
    "\n",
    "Before we start, ensure you have a GitHub account with access to the GDP Labs GenAI SDK GitHub repository. Then, follow these steps to create a personal access token:\n",
    "1. Log in to your [GitHub](https://github.com/) account.\n",
    "2. Navigate to the [Personal Access Tokens](https://github.com/settings/tokens) page.\n",
    "3. Select the `Generate new token` option. You can use the classic version instead of the beta version.\n",
    "4. Fill in the required information, ensuring that you've checked the `repo` option to grant access to private repositories.\n",
    "5. Save the newly generated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_sdk_library() -> None:\n",
    "    \"\"\"Installs the `gdplabs_gen_ai` library from a private GitHub repository using a Personal Access Token.\n",
    "\n",
    "    This function prompts the user to input their Personal Access Token for GitHub authentication. It then constructs\n",
    "    the repository URL with the provided token and executes a subprocess to install the library via pip from the\n",
    "    specified repository.\n",
    "\n",
    "    Raises:\n",
    "        subprocess.CalledProcessError: If the installation process returns a non-zero exit code.\n",
    "\n",
    "    Note:\n",
    "        The function utilizes `getpass.getpass()` to securely receive the Personal Access Token without echoing it.\n",
    "    \"\"\"\n",
    "    token = getpass.getpass(\"Input Your Personal Access Token: \")\n",
    "    repo_url_with_token = f\"https://{token}@github.com/GDP-ADMIN/gen-ai-internal.git\"\n",
    "    cmd = [\"pip\", \"install\", f\"gdplabs_gen_ai[eval] @ git+{repo_url_with_token}\"]\n",
    "\n",
    "    try:\n",
    "        with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "                              text=True, bufsize=1, universal_newlines=True) as process:\n",
    "            for line in process.stdout:\n",
    "                sys.stdout.write(line)\n",
    "\n",
    "            process.wait()  # Wait for the process to complete.\n",
    "            if process.returncode != 0:\n",
    "                raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}.\")\n",
    "\n",
    "install_sdk_library()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Warning:</b>\n",
    "After running the command above, you need to restart the runtime in Google Colab for the changes to take effect. Not doing so might lead to the newly installed libraries not being recognized.\n",
    "\n",
    "To restart the runtime in Google Colab:\n",
    "- Click on the `Runtime` menu.\n",
    "- Select `Restart runtime`.\n",
    "\n",
    "Once you have completed the previous step, you are ready to start the evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Contexts and Ground Truth Contexts\n",
    "Once you have completed the previous step, you can start by importing the necessary library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "from gdplabs_gen_ai.evaluation import EvaluateRetrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then prepare your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth contexts for each query with relevance scores. \n",
    "# Use `query_id` and `context_id` to represent the query and the contexts.\n",
    "ground_truth_contexts: Dict[str, Dict[str, int]] = {\n",
    "    \"query1\": {\"doc1\": 1, \"doc3\": 1, \"doc5\": 1},\n",
    "    \"query2\": {\"doc2\": 1, \"doc4\": 1},\n",
    "    \"query3\": {\"doc6\": 1, \"doc8\": 1, \"doc10\": 1},\n",
    "    \"query4\": {\"doc7\": 1, \"doc9\": 1}\n",
    "}\n",
    "\n",
    "# Retrieved contexts for each query with similarity scores. It doesn't need to be sorted by scores.\n",
    "# In this examples, we retrieve top 5 contexts for each query.\n",
    "retrieved_contexts: Dict[str, Dict[str, float]] = {\n",
    "    \"query1\": {\"doc1\": 0.3, \"doc2\": 0.9, \"doc3\": 0.8, \"doc4\": 0.2, \"doc5\": 0.7},\n",
    "    \"query2\": {\"doc1\": 0.8, \"doc2\": 0.3, \"doc3\": 0.6, \"doc4\": 0.2, \"doc5\": 0.4},\n",
    "    \"query3\": {\"doc6\": 0.5, \"doc7\": 0.3, \"doc8\": 0.7, \"doc9\": 0.2, \"doc10\": 0.6},\n",
    "    \"query4\": {\"doc1\": 0.8, \"doc3\": 0.8, \"doc5\": 0.5, \"doc7\": 0.3, \"doc9\": 0.2}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare list of k values that you want to evaluate (NDCG@k, MAP@k, Recall@k, Precision@k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of k values for evaluation.\n",
    "k_values: List[int] = [1, 3, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we prepare the dataset and k values, we can start the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: {'NDCG@1': 0.25, 'NDCG@3': 0.38268, 'NDCG@5': 0.68384}\n",
      "MAP: {'MAP@1': 0.08333, 'MAP@3': 0.34722, 'MAP@5': 0.57222}\n",
      "Recall: {'Recall@1': 0.08333, 'Recall@3': 0.41667, 'Recall@5': 1.0}\n",
      "Precision: {'P@1': 0.25, 'P@3': 0.41667, 'P@5': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate retrieval performance using NDCG@k, MAP@K, Recall@K, and Precision@K.\n",
    "ndcg, map_score, recall, precision = EvaluateRetrieval.evaluate(\n",
    "    ground_truth_contexts, retrieved_contexts, k_values\n",
    ")\n",
    "\n",
    "# Display the evaluation metrics.\n",
    "print(f\"NDCG: {ndcg}\")\n",
    "print(f\"MAP: {map_score}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "The output provides key insights into the performance of the retrieval system. Let's break down each metric and interpret what it signifies:  \n",
    "\n",
    "Normalized Discounted Cumulative Gain (NDCG):  \n",
    "NDCG@1: 0.25  \n",
    "NDCG@3: 0.38268  \n",
    "NDCG@5: 0.68384  \n",
    "Interpretation: NDCG measures the quality of the ranked retrieved documents based on their relevance. The values indicate that the relevance improves as more documents are considered (from 1 to 5). A score of 0.68384 at NDCG@5 suggests that the top 5 documents have a moderately high relevance. However, the lower score at NDCG@1 indicates that the most relevant document is not always ranked highest.  \n",
    "\n",
    "\n",
    "Mean Average Precision (MAP):  \n",
    "MAP@1: 0.08333  \n",
    "MAP@3: 0.34722  \n",
    "MAP@5: 0.57222  \n",
    "Interpretation: MAP assesses the precision across all queries. The scores reflect a similar trend as NDCG, where precision improves with more documents. The low score at MAP@1 implies that the top-ranked document is often not the most relevant. However, considering more documents (up to 5) increases the likelihood of including relevant documents.  \n",
    "\n",
    "\n",
    "Recall:  \n",
    "Recall@1: 0.08333  \n",
    "Recall@3: 0.41667  \n",
    "Recall@5: 1.0  \n",
    "Interpretation: Recall measures how many relevant documents are retrieved. The perfect score at Recall@5 indicates that all relevant documents are included within the top 5. However, the lower scores at Recall@1 and Recall@3 suggest that not all relevant documents are ranked at the very top.\n",
    "Precision:  \n",
    "P@1: 0.25  \n",
    "P@3: 0.41667  \n",
    "P@5: 0.5  \n",
    "Interpretation: Precision evaluates how many of the retrieved documents are relevant. The scores show that while half of the top 5 documents are relevant (P@5), the precision at the very top (P@1) is lower.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Based on Metrics:\n",
    "\n",
    "1. **Focus on Improving Top-Ranked Results**: The lower scores at NDCG@1 and MAP@1 suggest a need to enhance the algorithm to ensure the most relevant documents are ranked higher.  \n",
    "2. **Balance Between Precision and Recall**: While recall at 5 is perfect, precision is only 50%, indicating a trade-off. Aim to improve precision without significantly compromising recall.  \n",
    "3. **Analyze Individual Queries**: Investigate queries where the retrieval system underperforms, especially those contributing to lower scores at lower 'k' values.  \n",
    "4. **Refine Retrieval Algorithms**: Consider adjusting or fine-tuning the retrieval algorithm, possibly integrating more sophisticated techniques like semantic search or machine learning models that can better understand query intent.  \n",
    "5. **Iterative Testing and Evaluation**: Continuously evaluate and refine the system using different datasets and queries to ensure robustness and versatility.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "empty_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
