{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwABn_XL78IY"
      },
      "source": [
        "# Evaluate Relevance Between Ground Truth Responses and Retrieved Contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caw7ngLl8C8U"
      },
      "source": [
        "**Authors**: \n",
        "- Komang Elang Surya Prawira (komang.e.s.prawira@gdplabs.id)\n",
        "- Surya Mahadi (made.r.s.mahadi@gdplabs.id)\n",
        "\n",
        "**Reviewers**: \n",
        "- Novan Parmonangan Simanjuntak (novan.p.simanjuntak@gdplabs.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFpk43UE8Pnd"
      },
      "source": [
        "## References\n",
        "[1] [GDP Labs GenAI SDK - Evaluate Relevance Between Ground Truth Responses and Retrieved Contexts](#) \\\n",
        "[2] [LLamaIndex - Faithfullness Evaluator](https://docs.llamaindex.ai/en/stable/examples/evaluation/faithfulness_eval.html) \\\n",
        "[3] [Ragas - Evaluation](https://github.com/explodinggradients/ragas/blob/main/src/ragas/evaluation.py) \\\n",
        "[4] [Ragas - Context Recall](https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_context_recall.py) \\\n",
        "[5] [LangChain - OpenAI](https://python.langchain.com/docs/integrations/chat/openai)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Description\n",
        "\n",
        "In this notebook, we will explore how to evaluate the performance of our retrieval using the retrieved contexts and ground truth responses as references. We will leverage LLM to evaluate the retrieved contexts. Below is the data needed to perform this evaluation:\n",
        "1. QuestioN: Query used to get the retrieved contexts.\n",
        "2. Retrieved Contexts: Contexts retrieved for each question.\n",
        "3. Ground Truth Responses: Ground truth responses for each question.\n",
        "\n",
        "\n",
        "We utilize two metrics each from LlamaIndex and Ragas to calculate the score:\n",
        "1. **Faithfulness** measures the extent to which the retrieved contexts are correct based on the ground truth responses. \n",
        "2. **Context Recall** measures the extent to which the ground truth responses are reflected (mentioned) in the retrieved contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpnjCv8wCBtm"
      },
      "source": [
        "# Prepare Environment\n",
        "\n",
        "Before we start, ensure you have a GitHub account with access to the GDP Labs GenAI SDK GitHub repository. Then, follow these steps to create a personal access token:\n",
        "1. Log in to your [GitHub](https://github.com/) account.\n",
        "2. Navigate to the [Personal Access Tokens](https://github.com/settings/tokens) page.\n",
        "3. Select the `Generate new token` option. You can use the classic version instead of the beta version.\n",
        "4. Fill in the required information, ensuring that you've checked the `repo` option to grant access to private repositories.\n",
        "5. Save the newly generated token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D94Nd7mE9_An"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_sdk_library() -> None:\n",
        "    \"\"\"Installs the `gdplabs_gen_ai` library from a private GitHub repository using a Personal Access Token.\n",
        "\n",
        "    This function prompts the user to input their Personal Access Token for GitHub authentication. It then constructs\n",
        "    the repository URL with the provided token and executes a subprocess to install the library via pip from the\n",
        "    specified repository.\n",
        "\n",
        "    Raises:\n",
        "        subprocess.CalledProcessError: If the installation process returns a non-zero exit code.\n",
        "\n",
        "    Note:\n",
        "        The function utilizes `getpass.getpass()` to securely receive the Personal Access Token without echoing it.\n",
        "    \"\"\"\n",
        "    token = getpass.getpass(\"Input Your Personal Access Token: \")\n",
        "    repo_url_with_token = f\"https://{token}@github.com/GDP-ADMIN/gen-ai-internal.git@f/retrieval_evaluator\"\n",
        "    cmd = [\"pip\", \"install\", \"-e\", f\"git+{repo_url_with_token}#egg=gdplabs_gen_ai[eval]\"]\n",
        "\n",
        "    try:\n",
        "        with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                              text=True, bufsize=1, universal_newlines=True) as process:\n",
        "            for line in process.stdout:\n",
        "                sys.stdout.write(line)\n",
        "\n",
        "            process.wait()  # Wait for the process to complete.\n",
        "            if process.returncode != 0:\n",
        "                raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}.\")\n",
        "\n",
        "install_sdk_library()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gnTPpxPCeP0"
      },
      "source": [
        "<b>Warning:</b>\n",
        "After running the command above, you need to restart the runtime in Google Colab for the changes to take effect. Not doing so might lead to the newly installed libraries not being recognized.\n",
        "\n",
        "To restart the runtime in Google Colab:\n",
        "- Click on the `Runtime` menu.\n",
        "- Select `Restart runtime`.\n",
        "\n",
        "Once you have completed the previous step, you are ready to start the evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-mbsYlCgiO"
      },
      "source": [
        "# Faithfullness Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i3TCiPGz925V"
      },
      "outputs": [],
      "source": [
        "from gdplabs_gen_ai.evaluation import FaithfulnessEvaluator\n",
        "from llama_index import ServiceContext\n",
        "from llama_index.llms import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvFcpSOYCrJa"
      },
      "source": [
        "## Prepare Data\n",
        "You need to prepare your data in the following format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zs5VLIf496Jq"
      },
      "outputs": [],
      "source": [
        "ground_truth_responses = [\n",
        "    \"AI is artificial intelligence\",\n",
        "    \"Car is a transportation\",\n",
        "]\n",
        "retrieved_contexts = [\n",
        "    [\"Today AI is used everywhere.\", \"AI was first developed on 1970, AI stands for Artificial Intelligence.\"],\n",
        "    [\"Toyota is a car factory that success in Japan.\", \"Today lot of people use car as their main transportation.\"]\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UULaHhT6C2UW"
      },
      "source": [
        "## Set Up LLM and Evaluator\n",
        "Next, you need to define the LLM. In this example, we will use `GPT-4` as the LLM. Remember to put your `OPENAI_API_KEY` into the environment variables, you can use `os.environ` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PF5X3okm-egG"
      },
      "outputs": [],
      "source": [
        "# Create service context.\n",
        "gpt4 = OpenAI(temperature=0, model=\"gpt-4\")\n",
        "service_context_gpt4 = ServiceContext.from_defaults(llm=gpt4)\n",
        "\n",
        "# Create evaluator.\n",
        "evaluator_gpt4 = FaithfulnessEvaluator(service_context=service_context_gpt4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate the Score\n",
        "Finally, you can calculate the `Faithfullness` score using the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eMkBIVfx__z-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score: 1.0\n"
          ]
        }
      ],
      "source": [
        "scores = []\n",
        "\n",
        "for ground_truth_response, retrieved_context in zip(ground_truth_responses, retrieved_contexts):\n",
        "  # There are 2 API for evaluation, sync and async.\n",
        "  # In this example we use the async version.\n",
        "  result = await evaluator_gpt4.aevaluate(response=ground_truth_response, contexts=retrieved_context)\n",
        "  # If you want to use the sync version, you can use the following code.\n",
        "  # result = evaluator_gpt4.evaluate(response=ground_truth_response, contexts=retrieved_context)\n",
        "  \n",
        "  scores.append(int(result.passing))\n",
        "\n",
        "print(f\"Score: {sum(scores)/len(scores)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMfeCsqVDLrl"
      },
      "source": [
        "The above example will calculate each ground truth response-context pair, either `PASS` or `NOT PASS` and than calculate the mean score.\n",
        "\n",
        "**Note:** Since in this example we use Jupyter Notebook, and internally [Jupyter Notebook already running an event loop](https://blog.jupyter.org/ipython-7-0-async-repl-a35ce050f7f7), you can only use the async version here. If you want to use the sync version, use it outside Jupyter Notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Context Recall Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from ragas.llms import LangchainLLM\n",
        "\n",
        "from gdplabs_gen_ai.evaluation import evaluate, ContextRecall\n",
        "from gdplabs_gen_ai.evaluation.utility import convert_to_hf_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Data\n",
        "You need to prepare your data in the following format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['retrieved_contexts', 'questions', 'ground_truth_responses'],\n",
            "    num_rows: 2\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Define your data here before converting it into a Hugging Face's `Dataset` object.\n",
        "retrieved_contexts = [[\"AI is dangerous\", \"Artificial Intelligence is hard to master\"], \n",
        "                      [\"Elon Musk is rich\", \"CEO of SpaceX is Elon Musk\"]]\n",
        "questions = [\"What is AI?\", \"Who is Elon Musk?\"]\n",
        "ground_truth_responses = [[\"A field of computer science\"],\n",
        "                          [\"An entrepreneur and business magnate\"]]\n",
        "\n",
        "dataset = convert_to_hf_dataset(retrieved_contexts, questions=questions, ground_truth_responses=ground_truth_responses)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up LLM and Evaluator\n",
        "Next, you need to define the LLM. In this example, we will use `GPT-4` as the LLM. Remember to put your `OPENAI_API_KEY` into the environment variables, you can use `os.environ` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt4 = ChatOpenAI(model_name=\"gpt-4\")\n",
        "gpt4_wrapper = LangchainLLM(llm=gpt4)\n",
        "\n",
        "context_recall = ContextRecall(\n",
        "    batch_size=10\n",
        ")\n",
        "context_recall.llm = gpt4_wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate the Score\n",
        "Finally, you can calculate the `ContextRecall` score using the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [context_recall]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:10<00:00, 10.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 0.0000}\n"
          ]
        }
      ],
      "source": [
        "score_gpt4 = evaluate(\n",
        "    dataset,\n",
        "    metrics=[context_recall],\n",
        "    column_map={\"contexts\": \"retrieved_contexts\", \"question\": \"questions\", \"ground_truths\": \"ground_truth_responses\"},\n",
        ")\n",
        "\n",
        "print(score_gpt4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
