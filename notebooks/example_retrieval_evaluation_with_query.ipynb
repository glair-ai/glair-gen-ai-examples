{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwABn_XL78IY"
      },
      "source": [
        "# Example of Evaluate Relevance Between Query and Retrieved Contexts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caw7ngLl8C8U"
      },
      "source": [
        "**Authors**: \n",
        "- Komang Elang Surya Prawira (komang.e.s.prawira@gdplabs.id)\n",
        "\n",
        "**Reviewers**: \n",
        "- Novan Parmonangan Simanjuntak (novan.p.simanjuntak@gdplabs.id)\n",
        "- Surya Mahadi (made.r.s.mahadi@gdplabs.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFpk43UE8Pnd"
      },
      "source": [
        "## References\n",
        "[1] [GDP Labs GenAI SDK - Evaluate Relevance Between Query and Retrieved Contexts](#) \\\n",
        "[2] [Ragas - Evaluation](https://github.com/explodinggradients/ragas/blob/main/src/ragas/evaluation.py) \\\n",
        "[3] [Ragas - Context Precision](https://github.com/explodinggradients/ragas/blob/main/src/ragas/metrics/_context_precision.py) \\\n",
        "[4] [LangChain - OpenAI](https://python.langchain.com/docs/integrations/chat/openai)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Description\n",
        "\n",
        "In this notebook, we will explore how to evaluate the performance of our retrieval using the retrieved contexts and ground truth responses as references. We will leverage LLM to evaluate the retrieved contexts. Below is the data needed to perform this evaluation:\n",
        "1. Question: Query used to get the retrieved contexts.\n",
        "2. Retrieved Contexts: Contexts retrieved for each question.\n",
        "\n",
        "We utilize two metrics each from LlamaIndex and Ragas to calculate the score:\n",
        "1. **Context Precision** measures the extent to which the retrieved contexts are relevant to the given question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpnjCv8wCBtm"
      },
      "source": [
        "# Prepare Environment\n",
        "\n",
        "Before we start, ensure you have a GitHub account with access to the GDP Labs GenAI SDK GitHub repository. Then, follow these steps to create a personal access token:\n",
        "1. Log in to your [GitHub](https://github.com/) account.\n",
        "2. Navigate to the [Personal Access Tokens](https://github.com/settings/tokens) page.\n",
        "3. Select the `Generate new token` option. You can use the classic version instead of the beta version.\n",
        "4. Fill in the required information, ensuring that you've checked the `repo` option to grant access to private repositories.\n",
        "5. Save the newly generated token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D94Nd7mE9_An"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_sdk_library() -> None:\n",
        "    \"\"\"Installs the `gdplabs_gen_ai` library from a private GitHub repository using a Personal Access Token.\n",
        "\n",
        "    This function prompts the user to input their Personal Access Token for GitHub authentication. It then constructs\n",
        "    the repository URL with the provided token and executes a subprocess to install the library via pip from the\n",
        "    specified repository.\n",
        "\n",
        "    Raises:\n",
        "        subprocess.CalledProcessError: If the installation process returns a non-zero exit code.\n",
        "\n",
        "    Note:\n",
        "        The function utilizes `getpass.getpass()` to securely receive the Personal Access Token without echoing it.\n",
        "    \"\"\"\n",
        "    token = getpass.getpass(\"Input Your Personal Access Token: \")\n",
        "    repo_url_with_token = f\"https://{token}@github.com/GDP-ADMIN/gen-ai-internal.git@f/retrieval_evaluator\"\n",
        "    cmd = [\"pip\", \"install\", \"-e\", f\"git+{repo_url_with_token}#egg=gdplabs_gen_ai[eval]\"]\n",
        "\n",
        "    try:\n",
        "        with subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
        "                              text=True, bufsize=1, universal_newlines=True) as process:\n",
        "            for line in process.stdout:\n",
        "                sys.stdout.write(line)\n",
        "\n",
        "            process.wait()  # Wait for the process to complete.\n",
        "            if process.returncode != 0:\n",
        "                raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}.\")\n",
        "\n",
        "install_sdk_library()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gnTPpxPCeP0"
      },
      "source": [
        "<b>Warning:</b>\n",
        "After running the command above, you need to restart the runtime in Google Colab for the changes to take effect. Not doing so might lead to the newly installed libraries not being recognized.\n",
        "\n",
        "To restart the runtime in Google Colab:\n",
        "- Click on the `Runtime` menu.\n",
        "- Select `Restart runtime`.\n",
        "\n",
        "Once you have completed the previous step, you are ready to start the evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Context Precision Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from ragas.llms import LangchainLLM\n",
        "\n",
        "from gdplabs_gen_ai.evaluation import evaluate, ContextPrecision\n",
        "from gdplabs_gen_ai.evaluation.utility import convert_to_hf_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Data\n",
        "You need to prepare your data in the following format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['retrieved_contexts', 'questions'],\n",
            "    num_rows: 2\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# Define your data here before converting it into a Hugging Face's `Dataset` object.\n",
        "retrieved_contexts = [\n",
        "    [\"Today AI is used everywhere.\", \"AI was first developed on 1970, AI stands for Artificial Intelligence.\"],\n",
        "    [\"Toyota is a car factory that success in Japan.\", \"Today lot of people use car as their main transportation.\"]\n",
        "]\n",
        "questions = [\"What is AI?\", \"What is a car?\"]\n",
        "\n",
        "dataset = convert_to_hf_dataset(retrieved_contexts, questions=questions)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up LLM and Evaluator\n",
        "Next, you need to define the LLM. In this example, we will use `GPT-4` as the LLM. Remember to put your `OPENAI_API_KEY` into the environment variables, you can use `os.environ` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "gpt4 = ChatOpenAI(model_name=\"gpt-4\")\n",
        "gpt4_wrapper = LangchainLLM(llm=gpt4)\n",
        "\n",
        "context_precision = ContextPrecision(\n",
        "    batch_size=10\n",
        ")\n",
        "context_precision.llm = gpt4_wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculate the Score\n",
        "Finally, you can calculate the `ContextPrecision` score using the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "evaluating with [context_precision]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:16<00:00, 16.86s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_precision': 0.2500}\n"
          ]
        }
      ],
      "source": [
        "score_gpt4 = evaluate(\n",
        "    dataset,\n",
        "    metrics=[context_precision],\n",
        "    column_map={\"contexts\": \"retrieved_contexts\", \"question\": \"questions\"},\n",
        ")\n",
        "\n",
        "print(score_gpt4)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
